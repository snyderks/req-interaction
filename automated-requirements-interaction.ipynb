{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASN3 - Automated Requirements Interaction\n",
    "\n",
    "In order to analyze requirements interaction, we first need a list of requirements.\n",
    "Our requirements will be in the following format:\n",
    "\n",
    "```\n",
    "USX: User story title\n",
    "\n",
    "As a: target role\n",
    "I want to: perform an action\n",
    "So that: a goal is met\n",
    "```\n",
    "\n",
    "I'll use a class to store a story. There won't be many actions here; it'll mainly act as a named structure type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Story:\n",
    "    \"\"\"\n",
    "    Story contains all known data about a given user story.\n",
    "    \"\"\"\n",
    "    def __init__(self, number, title, role, action, goal):\n",
    "        self.number = number\n",
    "        self.title = title\n",
    "        self.role = role\n",
    "        self.action = action\n",
    "        self.goal = goal\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\"Story \" + str(self.number) + \": \" + self.title +\n",
    "               \" As a \" + self.role + \", I would like to \" + self.action +\n",
    "               \" so that: \" + self.goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I set up the regex to parse the file (in the above formats) with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This set of regex matches the format above, \n",
    "# extracting (each section corresponds to a captured element)\n",
    "# 1. The number (X above)\n",
    "# 2. The story's title\n",
    "# 3. User role, action, or goal\n",
    "re_num = \"(?:US)(\\d+)\"\n",
    "re_title = \"(?:\\W*)(.+)\"\n",
    "re_detail = \"(?:[\\w\\s]+:\\s*)(.+)\"\n",
    "# Each index of the array will be what we search for.\n",
    "searches = [re_num + re_title, re_detail, re_detail, re_detail]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up the file for parsing and get an array of stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "stories = []\n",
    "\n",
    "curr_story = []\n",
    "index = 0\n",
    "\n",
    "\n",
    "with open(\"stories.txt\") as s:\n",
    "    for line in s:\n",
    "        # can't capture anything\n",
    "        if len(line) is 0 or line.isspace():\n",
    "            pass\n",
    "        # try and parse the line\n",
    "        else:\n",
    "            r = re.search(searches[index], line)\n",
    "            if r == None:\n",
    "                sys.exit(\"Failed to parse line.\")\n",
    "            curr_story.append(r.groups(0)[0])\n",
    "            if index == 0:\n",
    "                curr_story.append(r.groups(1)[1])\n",
    "            index += 1\n",
    "            if index >= 4:\n",
    "                stories.append(Story(int(curr_story[0]), *curr_story[1:]))\n",
    "                index = 0\n",
    "                curr_story = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an array of stories. Here's an example of the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story 1: User abilities - super-duper-user As a system user, I would like to be a super-duper-user so that: I have access to all system abilities, including the ability to view restricted content, and to assign roles\n"
     ]
    }
   ],
   "source": [
    "print(stories[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of text, but it doesn't tell us much.\n",
    "\n",
    "---\n",
    "\n",
    "## Natural Language Processing\n",
    "For this task (and lots of others), I'll be employing a library called [spaCy](https://spacy.io). spaCy could be better referred to as an NLP toolkit, written in Cython. This allows it to take advantage of the simplicity and expressiveness of Python while still utilizing the blazing speed of compiled C.\n",
    "\n",
    "spaCy has pre-trained neural networks and predefined rulesets for the English language (and others) that can analyze a body of text or small snippets, including computing similarity between two strings.\n",
    "\n",
    "This similarity will be the main method by which I will be looking at interaction.\n",
    "\n",
    "Let's take a look at the action for User Story 1 compared with a few of the others (including itself!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US1: be a super-duper-user compared with:\n",
      "0.999999987867 (US1): be a super-duper-user \n",
      "\n",
      "0.360403269612 (US2): be able to designate roles to user accounts \n",
      "\n",
      "0.903838203828 (US3): be a super-user \n",
      "\n",
      "0.428586439198 (US4): add users to a workgroup \n",
      "\n",
      "0.361153475569 (US5): hide objects \n",
      "\n",
      "0.372441500027 (US6): set different levels of editing rights based on individual user(s) \n",
      "\n",
      "0.311382344599 (US7): embargo content \n",
      "\n",
      "0.425561429019 (US8): have access to the master files \n",
      "\n",
      "0.442165713863 (US9): designate public access levels for workgroup content once it is published \n",
      "\n",
      "0.216929744028 (US10): control which users can send submissions to me \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') # using this model for better precomputed vectors\n",
    "\n",
    "parsed = [nlp(s.action) for s in stories]\n",
    "\n",
    "print(\"US\" + str(stories[0].number) + \":\", parsed[0], \"compared with:\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(parsed[0].similarity(parsed[i]), \"(US\" + str(stories[i].number) + \"):\", parsed[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The [similarity](https://spacy.io/api/doc#similarity) method in spaCy is computed using a [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity) between the multidimensional vectors for each word.\n",
    "\n",
    "spaCy utilizes a 4-layer convolutional neural network, pretrained, in this case, on a robust corpus of the English language with [word2vec](https://en.wikipedia.org/wiki/Word2vec). Due to the multilayer CNN, it relies heavily on context to compute word vectors and, therefore, I will not be removing stop words from the input to prevent confusion by the system about phrase structure within the stories.\n",
    "\n",
    "Further reading on the systems employed by spaCy in this situation is available [here](https://spacy.io/usage/vectors-similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Process of Determining Interaction\n",
    "\n",
    "I will be relying heavily on the similarity score provided by spaCy to determine whether two stories interact.\n",
    "Based on the answer set to the test data, **I will determine an appropriate weight to the scores provided by both the goal and action sections of the stories, combining them both to reach a binary result of interaction or no interaction.** The title and number will not have a bearing on interaction.\n",
    "\n",
    "## Determining Relevancy to the Resource\n",
    "\n",
    "Not all interactions, however, are relevant to the resource being selected. Therefore, I will also compute a similarity score of the word or phrase to the same components as previous, hand-adjusting the sensitivity based on solution data to balance inclusion of relevant stories with exclusion of the irrelevant.\n",
    "\n",
    "## Results\n",
    "\n",
    "After the above is completed, it is trivial to output a list of pairs for checking against an answer set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
